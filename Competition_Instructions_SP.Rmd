---
title: "TSA: Forecasting Competition Instructions"
author: "Sai Powar & Vinny Whatley"
date: "03/20/2024"
output: pdf_document
always_allow_html: true
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = TRUE,tidy.opts=list(width.cutoff=80), tidy=FALSE) 
```

## CREATE A REPOSITORY IN YOUR GITHUB ACCOUNT

1. Go to your user account on GitHub an navigate to the repositories tab. 

3. In the upper right corner, click the green "New" button. 

4. Name your repository with recommended naming conventions (suggestion: *Lastname1Lastname2_ENV797_TSA_Competition_S2024*). Write a short description of the purpose of the repository. Check the box to initialize the repository with a README. Add a .gitignore for R and add a GNU General Public License v3.0.

5. Invite other group members as collaborators to the repository.

## LINK YOUR REPO TO YOUR LOCAL DRIVE WITH RSTUDIO
 
1. Click the "Clone or download" button for your repository and then the "copy" icon. Make sure the box header lists "Clone with HTTPS" rather than "Clone with SSH." If not, click the "Use HTTPS" button and then copy the link.

2. Launch RStudio and select "New Project" from the File menu. Choose "Version Control" and "Git."

3. Paste the repository URL and give your repository a name and a file path.

## ENTER THE KAGGLE COMPETITION

I created a kaggle competition for this assignment. This is not a public competition, it's an invitation only competition. You will need to enter the competition using this
[invitation](https://www.kaggle.com/t/7c42475e0e62412ea7e7ffb30affee1f).  

## DOWNLOAD THE DATASET

You should download the data set from Kaggle platform. You will find three datasets one with hourly demand, one with hourly temperature and one with hourly relative humidity from January 2005 to June 2011. Note the way the data is presented is different.

Your goal is to forecast **daily** load for July 2011 based on this historical data. You may or may not use the temperature and relative humidity in your models. The temperature and humidity measurement are from stations close to the household meter data you have.

## WRANGLE/PROCESS THE DATASET

You will need to transform hourly data into daily data. See the Rmd file from Lesson 11 for instruction on how to aggregate your dataset using pipes. You should take the **average** of the 24 hours to obtain the daily averages.

```{r package, message=FALSE}
library(lubridate)
library(ggplot2)
library(forecast)  
library(Kendall)
library(tseries)
library(outliers)
library(tidyverse)
library(cowplot)
#install.packages("smooth")
library(smooth)
#install.packages("kableExtra")
library(kableExtra)
```

```{r}
#importing datasets

raw_load <- read.csv(file="./Data_TOPOST/load.csv", header = TRUE, dec = ".", sep=",") #na.strings = c("","NA")) 
raw_temp <- read.csv(file="./Data_TOPOST/temperature.csv", header = TRUE, dec = ".", sep=",")
raw_humidity <- read.csv(file="./Data_TOPOST/relative_humidity.csv", header = TRUE, dec = ".", sep=",")
```

```{r}
#cleaning load dataset
raw_load <- raw_load[,-1]
```

```{r}
#cleaning more and calculating daily averages

raw_load_pivot <- raw_load %>%
  pivot_longer(cols=h1:h24, names_to = "Hour", values_to = "Load") %>%
  mutate(date = mdy(date))

load <- raw_load_pivot %>%
  filter(!is.na(Load)) %>%
  group_by(date) %>%
  summarise(daily_load = mean(Load))

#raw_load_2<- raw_load %>%
  #drop_na() %>%
  #mutate(date = mdy(date)) %>%
  #mutate_at(2:25, ~ as.numeric(.))

#load_1 <- raw_load_2 %>%
  #mutate(daily_load = rowMeans(raw_load_2[,2:25], na.rm=TRUE))

#load_1 <- load_1[,c(1,26)]

#load_1 <- load_1 %>%
  #filter(!is.na(daily_load))

```

```{r}
#cleaning and calculating daily temp averages

raw_temp_pivot <- raw_temp %>%
  pivot_longer(cols=t_ws1:t_ws28, names_to = "station", values_to = "temp") %>%
  mutate(date = dmy(date))

temp <- raw_temp_pivot %>%
  filter(!is.na(temp)) %>%
  group_by(date) %>%
  summarise(daily_temp = mean(temp)) %>%
  drop_na()

#cleaning and calculating daily humidity

raw_hum_pivot <- raw_humidity %>%
  pivot_longer(cols=rh_ws1:rh_ws28, names_to = "station", values_to = "humidity") %>%
  mutate(date = dmy(date))

humidity <- raw_hum_pivot %>%
  filter(!is.na(humidity)) %>%
  group_by(date) %>%
  summarise(daily_humidity = mean(humidity))

#old code
#raw_temp_2 <- raw_temp %>%
  #drop_na() %>%
  #mutate(date = dmy(date)) %>%
  #mutate_at(3:30, ~ as.numeric(.))

#temp <- raw_temp_2%>%
  #mutate(hourly_avg = rowMeans(raw_temp_2[,3:30], na.rm=TRUE))%>%
  #group_by(date) %>%
  #summarise(daily_temp=mean(hourly_avg))

#raw_humidity_2 <- raw_humidity %>%
  #drop_na() %>%
  #mutate(date = dmy(date)) %>%
  #mutate_at(3:30, ~ as.numeric(.))

#humidity <- raw_humidity_2%>%
  #mutate(hourly_avg = rowMeans(raw_humidity_2[,3:30], na.rm=TRUE))%>%
  #group_by(date) %>%
  #summarise(daily_humidity=mean(hourly_avg))

```


## CREATE A TIME SERIES OBJECT

After you process your dataset use the `msts()` function to create a time series object. You need to use `msts()` instead of `ts()` because your daily data will have more than one seasonal component.After you process your dataset use the `msts()` function to create a time series object.

```{r}
# Create a time series object using msts()
ts_load <- msts(load$daily_load, seasonal.periods = c(7,365.25), start = c(2005,1,1))
head(ts_load)

ts_temp <- msts(temp$daily_temp, seasonal.periods = c(7,365.25), start = c(2005,1,1))

ts_humidity <- msts(humidity$daily_humidity, seasonal.periods = c(7,365.25), start = c(2005,1,1))
```


```{r}

#plotting the time series

autoplot(ts_load,ylab="Daily Load")

plot_grid(
  autoplot(ts_temp, ylab="Daily Temp"),
  autoplot(ts_humidity, ylab="Daily Humidity"))


```


## FIT MODELS TO YOUR DATA

Fit models to your dataset considering the period Jan 1st 2005 to May 31st 2011. 

```{r}

#plotting ACF and PACF

plot_grid(
  autoplot(Acf(ts_load,lag.max=40,plot=FALSE)),
  autoplot(Pacf(ts_load,lag.max=40,plot=FALSE)))

par(mfrow=c(1,2))
ACF_Plot <- Acf(ts_load, lag = 40, plot = TRUE)
PACF_Plot <- Pacf(ts_load, lag = 40)
par(mfrow=c(1,1))

#ACF has slow decats_load#ACF has slow decay, and PACF cuts off at lag 1 and is negative = AR process
#there are spikes at seasonal lags as well

```

```{r}
#trend tests
SMKtest <- SeasonalMannKendall(ts_load)
print("Results for Seasonal Mann Kendall /n")
print(summary(SMKtest))

print("Results for ADF test/n")
print(adf.test(ts_load,alternative = "stationary"))

#SMK - S is positive, so there is an increasing trend. p-value<0.05 so there is a significant trend.
#ADF - p-value<0.05 so null hypothesis is rejected and series has deterministic/stationary trend. 

```

```{r}
#decomposing the series
ts_load %>% mstl() %>%
  autoplot()

#increasing trend, seasonality too but the wave pattern is interesting

```


```{r}

#creating subset for training purposes

n_for = 30

ts_load_train <- subset(ts_load,end = length(ts_load)-n_for)
ts_load_test <- subset(ts_load,start= length(ts_load)-n_for+1)

autoplot(ts_load_train)
autoplot(ts_load_test)
```


```{r}

#not using SARIMA
#Model 3: SARIMA
#Model3 <- auto.arima(ts_load_train)

#Model 4: SES 
Model4 <- ses(ts_load_train, h=n_for)
plot(Model4)

#Model 5: SS Exponential smoothing 
Model5 <- es(ts_load_train, model="ZZZ",h=n_for, holdout = FALSE)
plot(Model5)
checkresiduals(Model5)

#Model 6: STL + ETS
Model6<- stlf(ts_load_train,h=n_for)
plot(Model6)

#Model 7: ARIMA + Fourier K=2,12
Model7 <- auto.arima(ts_load_train, 
                             seasonal=FALSE, 
                             lambda=0,
                             xreg=fourier(ts_load_train, 
                              K=c(2,12)))

#Model 8: ARIMA + Fouriier K=2,24
Model8 <- auto.arima(ts_load_train, 
                             seasonal=FALSE, 
                             lambda=0,
                             xreg=fourier(ts_load_train, 
                              K=c(2,24)))

# Model9
Model9 <- snaive(ts_load_train)
checkresiduals(Model9)

#Model10
Model10 <- nnetar(ts_load_train,
                  p=1,
                  P=0,
                  xreg=fourier(ts_load_train, K=c(2,12)))

#Model12
Model12 <- stl(ts_load_train, s.window = "periodic", robust = TRUE)



```

## FORECAST DAILY DEMAND FOR JUNE 2011 

Using the models from previous section, forecast daily demand for the period June 1st 2011 to June 30th 2011. Based on the models you developed which model(s) is(are) generating good forecasts? Use performance criteria MAPE (Mean Average Percentage Error) to rank/select models.

You may use different forecasting windows for model selection. For example you might want to see the model behavior when forecasting July of 2010. 

```{r}

#not using SARIMA anymore
#forecasting using SARIMA
#Model3_for_jun <- forecast(Model3, h=n_for)
#plot(Model3_for_jun)

#autoplot(ts_load,series="Original")+
  #autolayer(Model3$fitted,series="SARIMA")+
  #autolayer(Model3_for_jun$mean,series = "SARIMA forecast")
  #xlim(2010,NA)

#SARIMA_score <- accuracy(Model3_for_jun$mean,ts_load_test)

#SES
SES_score <- accuracy(Model4$mean,ts_load_test)

#SS ES
SSES_score <- accuracy(Model5$forecast, ts_load_test)

#ETS score
ETS_score <- accuracy(Model6$mean, ts_load_test)

#ARIMA + fourier, K=(2,12)
Model7_for <- forecast(Model7,
                           xreg=fourier(ts_load_train,
                                        K=c(2,12),
                                        h=n_for), #forecast 365 fourier terms
                           h=n_for #forecast 365 terms for the arima model 
                           )
autoplot(Model7_for)

Model7_score <- accuracy(Model7_for$mean, ts_load_test)

#ARIMA + fourier, K=(2,24)
Model8_for <- forecast(Model8,
                           xreg=fourier(ts_load_train,
                                        K=c(2,24),
                                        h=n_for), #forecast 365 fourier terms
                           h=n_for #forecast 365 terms for the arima model 
                           )
autoplot(Model8_for)

Model8_score <- accuracy(Model8_for$mean, ts_load_test)

#Snaive
Model9_for <- forecast(Model9, h = n_for)
plot(Model9_for)
Model9_score <- accuracy(Model9_for$mean, ts_load_test)

#NN K=2,12
Model10_for <- forecast(Model10, h=n_for,xreg=fourier(ts_load_train,K=c(2,12),h=n_for))
autoplot(Model10_for)
Model10_score <- accuracy(Model10_for$mean, ts_load_test)

#STL
Model12_for <- forecast(Model12, h = n_for)
autoplot(Model12_for)
Model12_score <- accuracy(Model12_for$mean, ts_load_test)

```


```{r}
#creating data frame of all the model scores
all_scores <- as.data.frame(rbind(SES_score, SSES_score, ETS_score, Model7_score, Model8_score, Model9_score, Model10_score, Model12_score))
row.names(all_scores) <- c("SES", "SSES", "STL+ETS", "A+F,K=c(2,12)", "A+F,K=c(2,24)", "SNaive", "NN K=c(2,12)", "STL")

```

```{r}

#table of scores

kbl(all_scores,
    caption = "Forecast accuracy for daily load data",
    digits = array(5,ncol(all_scores)))%>%
  kable_styling(full_width = FALSE, position = "center") %>%
  kable_styling(latex_options="striped", stripe_index = which.min(all_scores[,"MAPE"]))

```

## FORECAST DAILY DEMAND FOR JULY 2011

Just for the good/selected model(s) you will **re-run** the model but now using the entire dataset all the way to June 30th 2011. Then forecast Jan 1st 2011 to Jan 31st 2011.

```{r}
#only competition submissions

#Model 1: Arithmetic mean
Model1 <- meanf(ts_load,h=n_for+1)
summary(Model1)
plot(Model1)

#Model 2: Seasonal naive
Model2 <- snaive(ts_load,h=n_for+1)
summary(Model2)
plot(Model2)

#Model 3: SARIMA
SARIMA_jul <- auto.arima(ts_load)
SARIMA_for_jul <- forecast(SARIMA_jul, h=n_for+1)
plot(SARIMA_for_jul)

#Model 4: SES
SES_jul <- ses(ts_load, h=n_for+1)
plot(SES_jul)

#Model 5: SSES
SSES_jul <- es(ts_load, model="ZZZ",h=n_for+1, holdout = FALSE)

#Model 6: STL + ETS
ETS_jul <- stlf(ts_load,h=n_for+1)
plot(ETS_jul)

#Model 7: ARIMA + Fourier
AF12_jul <- auto.arima(ts_load, 
                             seasonal=FALSE, 
                             lambda=0,
                             xreg=fourier(ts_load, 
                              K=c(2,12)))

AF12_jul_for <- forecast(AF12_jul,
            xreg=fourier(ts_load,
            K=c(2,12),
            h=n_for+1),
            h=n_for+1 
            )
plot(AF12_jul_for)

#Model 8: ARIMA + Fourier
AF24_jul <- auto.arima(ts_load, 
                             seasonal=FALSE, 
                             lambda=0,
                             xreg=fourier(ts_load, 
                              K=c(2,24)))

AF24_jul_for <- forecast(AF24_jul,
            xreg=fourier(ts_load,
            K=c(2,24),
            h=n_for+1),
            h=n_for+1 
            )
plot(AF24_jul_for)

#Model 9: SNaive
SN_jul <- snaive(ts_load)
SN_jul_for <- forecast(SN_jul, h = n_for+1)
plot(SN_jul_for)

#Model 10: NN K=c(2,12)
NN12_jul <- nnetar(ts_load,
                  p=1,
                  P=0,
                  xreg=fourier(ts_load, K=c(2,12)))
NN12_jul_for <- forecast(NN12_jul, h=n_for+1,xreg=fourier(ts_load,K=c(2,12),h=n_for+1))
autoplot(NN12_jul_for)

#Model 12: STL
STL_jul <- stl(ts_load, s.window = "periodic", robust = TRUE)
STL_jul_for <- forecast(STL_jul, h = n_for+1)
autoplot(STL_jul_for)

```

## CREATE AN EXCEL FILE WITH FORECAST

Look at the excel file in your Output folder name "submission_template.csv". You will need to create your own output file with forecast for July 2011. Your file needs to be in the format of the submission template. If your forecast is a probability distribution function, consider the mean to be the point forecast.

```{r}
#creating date dataframe
date_for <- seq(from=as.Date("07-01-2011", format="%m-%d-%Y"), to=as.Date("07-31-2011",format="%m-%d-%Y"),by="1 day")
date_for <- as.data.frame(date_for)
colnames(date_for)="date"
```

```{r}
#only competition submissions

#creating Model 1 csv
amean_for <- data.frame(date=date_for,load=round(Model1$mean,digits=0))
write.csv(amean_for, file="./Output/submission_arithmetic_mean.csv", row.names = FALSE)

#creating Model 2 csv
snaive_for <- data.frame(date=date_for,load=round(Model2$mean,digits=0))
write.csv(snaive_for, file="./Output/submission_seasonal_naive.csv", row.names = FALSE)

#creating SARIMA csv
SARIMA_for <- data.frame(date=date_for,load=round(SARIMA_for_jul$mean,digits=0))
write.csv(SARIMA_for, file="./Output/submission_sarima.csv", row.names = FALSE)

#creating SES csv
SES_for <- data.frame(date=date_for,load=round(SES_jul$mean,digits=0))
write.csv(SES_for, file="./Output/submission_ses.csv", row.names = FALSE)

#creating SSES csv
SSES_for <- data.frame(date=date_for,load=round(SSES_jul$forecast,digits=0))
write.csv(SSES_for, file="./Output/submission_sses.csv", row.names = FALSE)

#creating ETS csv
ETS_for <- data.frame(date=date_for,load=round(ETS_jul$mean,digits=0))
write.csv(ETS_for, file="./Output/submission_ets.csv", row.names = FALSE)

#creating A+F K=2,12 csv
AF12_for <- data.frame(date=date_for,load=round(AF12_jul_for$mean,digits=0))
write.csv(AF12_for, file="./Output/submission_af12.csv", row.names = FALSE)

#creating A+F K=2,24 csv
AF24_for <- data.frame(date=date_for,load=round(AF24_jul_for$mean,digits=0))
write.csv(AF24_for, file="./Output/submission_af24.csv", row.names = FALSE)

#creating SNaive csv
SN_for <- data.frame(date=date_for,load=round(SN_jul_for$mean,digits=0))
write.csv(SN_for, file="./Output/submission_sn_vw.csv", row.names = FALSE)

#creatin NN12 csv
NN12_for <- data.frame(date=date_for,load=round(NN12_jul_for$mean,digits=0))
write.csv(NN12_for, file="./Output/submission_nn12.csv", row.names = FALSE)

#creating STL csv
STL_for <- data.frame(date=date_for,load=round(STL_jul_for$mean,digits=0))
write.csv(STL_for, file="./Output/submission_stl.csv", row.names = FALSE)
```

## SUBMIT FORECAST TO KAGGLE

You will need to submit your group's solution using this [link][
https://www.kaggle.com/competitions/tsa-s24-competition].

## COMPLETE YOUR PROJECT REPORT

For the project report you only need to organize your current Rmd file. Make sure you follow the guidelines and you provide a link to you Github repository.

1. Write in scientific style, not narrative style.

2. [Global options for R chunks](https://rmarkdown.rstudio.com/lesson-3.html) should be set so that only relevant output is displayed. Turn on/off messages and warnings when applicable to avoid unnecessary outputs on the pdf.

3. Make sure your final knitted PDF looks professional. Format tables, size figures, chapters, etc.

4. Make sure the PDF file has the file name "Lastname1Lastname2_ENV797_A08_Competition.pdf" and submit it to Sakai under A08 - part III. You will only submit your PDF file.


## GRADING RUBRIC

You will be graded based on how much time and effort you put into the competition and your ability to fit a model to the data set. More specifically I will look into:

1. number of commitments to Github repo, this item will show how the team interacted and how much you worked on the competition;

2. number of submissions to Kaggle platform, this will show how many models you tried and it's also an indication of how much effort the team put into the project;

3. ability to beat the vanilla/benchmark model, this will show your forecasting skills. 

The team that is leading the board when the competition ends will get extra points, but they still need to get good scores on 1 and 2. 
