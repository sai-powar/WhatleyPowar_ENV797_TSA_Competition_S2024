---
title: "TSA: Forecasting Competition Instructions"
author: "Sai Powar & Vinny Whatley"
date: "03/20/2024"
output: pdf_document
always_allow_html: true
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = TRUE,tidy.opts=list(width.cutoff=80), tidy=FALSE) 
```

```{r package, message=FALSE}
library(dplyr)
library(KFAS)
library(tidyr)
library(lubridate)
library(ggplot2)
library(forecast)  
library(Kendall)
library(tseries)
library(outliers)
library(tidyverse)
library(cowplot)
library(nnet)
library(KernSmooth)
#install.packages("smooth")
library(smooth)
library(kableExtra)

install.packages("gbm")
library(gbm)
```

```{r}
#change file path for data as necessary
raw_load <- read.csv(file="Data_TOPOST/converting files to csv/load.csv", header = TRUE, dec = ".", sep=",") #na.strings = c("","NA")) 
raw_temp <- read.csv(file="Data_TOPOST/converting files to csv/temperature.csv", header = TRUE, dec = ".", sep=",")
raw_humidity <- read.csv(file="Data_TOPOST/converting files to csv/relative_humidity.csv", header = TRUE, dec = ".", sep=",")
```

```{r}
#most recent load tab



load_gather <- raw_load %>%
  pivot_longer(cols = 3:30, names_to = "Hour", values_to = "Load")

load_daily <- load_gather %>%
  filter( !is.na(Load)) %>%
  group_by(date) %>%
  summarise( daily_mean_load = mean(Load)) 
```


```{r}
#cleaning and calculating daily temp averages

raw_temp_pivot <- raw_temp %>%
  pivot_longer(cols=t_ws1:t_ws28, names_to = "station", values_to = "temp") %>%
  mutate(date = dmy(date))

temp <- raw_temp_pivot %>%
  filter(!is.na(temp)) %>%
  group_by(date) %>%
  summarise(daily_temp = mean(temp)) %>%
  drop_na()

#cleaning and calculating daily humidity

raw_hum_pivot <- raw_humidity %>%
  pivot_longer(cols=rh_ws1:rh_ws28, names_to = "station", values_to = "humidity") %>%
  mutate(date = dmy(date))

humidity <- raw_hum_pivot %>%
  filter(!is.na(humidity)) %>%
  group_by(date) %>%
  summarise(daily_humidity = mean(humidity))
```

```{r}
# Create a time series object using msts()
ts_load <- msts(load_daily$daily_mean_load, seasonal.periods = c(7,365.25), start = c(2005,1,1))
head(ts_load)
 
# Plot the time series
plot(ts_load, main = "Time Series Plot", xlab = "Time", ylab = "Value")

ts_temp <- msts(temp$daily_temp, seasonal.periods = c(7,365.25), start = c(2005,1,1))

ts_humidity <- msts(humidity$daily_humidity, seasonal.periods = c(7,365.25), start = c(2005,1,1))
```

```{r}

#plotting the time series

autoplot(ts_load,ylab="Daily Load")

plot_grid(
  autoplot(ts_temp, ylab="Daily Temp"),
  autoplot(ts_humidity, ylab="Daily Humidity"))


```


## FIT MODELS TO YOUR DATA

Fit models to your dataset considering the period Jan 1st 2005 to May 31st 2011. 

```{r}

#plotting ACF and PACF

plot_grid(
  autoplot(Acf(ts_load,lag.max=40,plot=FALSE)),
  autoplot(Pacf(ts_load,lag.max=40,plot=FALSE)))

par(mfrow=c(1,2))
ACF_Plot <- Acf(ts_load, lag = 40, plot = TRUE)
PACF_Plot <- Pacf(ts_load, lag = 40)
par(mfrow=c(1,1))

#ACF has slow decats_load#ACF has slow decay, and PACF cuts off at lag 1 and is negative = AR process
#there are spikes at seasonal lags as well

```


```{r}
# Subset data from January 1st, 2005 to May 31st, 2011
subset_data <- window(ts_load, start = c(2005, 1), end = c(2011, 6))

frequency(subset_data)

```


Fit models to your dataset considering the period Jan 1st 2005 to May 31st 2011. 

```{r}

#plotting ACF and PACF

plot_grid(
  autoplot(Acf(ts_load,lag.max=40,plot=FALSE)),
  autoplot(Pacf(ts_load,lag.max=40,plot=FALSE)))

ACF_Plot <- Acf(ts_load, lag = 40, plot = TRUE)
PACF_Plot <- Pacf(ts_load, lag = 40)

#ACF has slow decats_load#ACF has slow decay, and PACF cuts off at lag 1 and is negative = AR process
#there are spikes at seasonal lags as well

```

```{r}
#trend tests
SMKtest <- SeasonalMannKendall(ts_load)
print("Results for Seasonal Mann Kendall /n")
print(summary(SMKtest))

print("Results for ADF test/n")
print(adf.test(ts_load,alternative = "stationary"))

#SMK - S is positive, so there is an increasing trend. p-value<0.05 so there is a significant trend.
#ADF - p-value<0.05 so null hypothesis is rejected and series has deterministic/stationary trend. 

```


```{r}
#decomposing the series

decomposed <- decompose(ts_load)

seasonal <- decomposed$seasonal
trend <- decomposed$trend
random <- decomposed$random
```

```{r}

#creating subset for training purposes

n_for = 30

ts_load_train <- subset(ts_load,end = length(ts_load)-n_for)
ts_load_test <- subset(ts_load,start= length(ts_load)-n_for+1)

autoplot(ts_load_train)
autoplot(ts_load_test)
```



```{r}
#some of these take a while to load

# Model9
Model9 <- snaive(ts_load_train)
forecast_sn <- forecast(Model9, h = n_for)
plot(forecast_sn)
checkresiduals(Model9)



#Model 10 neural network model
Model10 <- nnetar(ts_load_train, h = n_for)
forecast_nn <- forecast(Model10)
autoplot(forecast_nn)




#fit previous two models to be combined into one model
seasonal_naive_model <- snaive(ts_load_train)
neural_network_model <- nnetar(ts_load_train)
n_for2 <- 12  # Adjust as needed

# orecasts from both models
forecast_seasonal_naive <- forecast(seasonal_naive_model, h = n_for2)
forecast_neural_network <- forecast(neural_network_model, h = n_for2)


# Check if both forecasts have non-zero length
if (length(forecast_seasonal_naive$mean) > 0 && length(forecast_neural_network$mean) > 0) {
  # Combine forecasts (for example, using equal weights)
  Model11 <- (forecast_seasonal_naive$mean + forecast_neural_network$mean) / 2
  
  # Plot combined forecast
  plot(Model11)
  
  # Check residuals
  # As neural networks are non-linear models, checking residuals may not be as straightforward as linear models.
  # You can consider visual inspection or other methods suitable for evaluating neural network forecasts.
} else {
  # Handle the case where one or both forecasts have zero length
  warning("One or both forecasts have zero length. Unable to combine forecasts.")
}

#Model 12 neural STL model
# Fit STL model
Model12 <- stl(ts_load_train, s.window = "periodic", robust = TRUE)
# Forecast
forecast_stl <- forecast(Model12, h = n_for)
# Plot forecast
autoplot(forecast_stl)


```

Submission
```{r}
#creating date dataframe
date_for <- seq(from=as.Date("07-01-2011", format="%m-%d-%Y"), to=as.Date("07-31-2011",format="%m-%d-%Y"),by="1 day")
date_for <- as.data.frame(date_for)
colnames(date_for)="date"
```

```{r}
length(date_for)
length(Model9$mean)

start_date <- as.Date("2011-07-01")  # Replace with your start date
end_date <- as.Date("2011-07-31")    # Replace with your end date
date_for <- seq(start_date, by = "day", length.out = length(Model9$mean))

str(Model10$mean)


str(Model11)
# Create SN_NN_for data frame
#SN_NN_for <- data.frame(date = SN_NN_model$date, load = SN_NN_model$load)

# Create STL_for data frame

STL_for <- data.frame(date = date_for, load = round(forecast_stl$mean, digits = 0))
length(date_for)
length(forecast_stl$mean)
```


```{r}
#only competition submissions

#creating Model 9 csv
Sn_for <- data.frame(date=date_for,load=round(forecast_sn$mean,digits=0))
write.csv(Sn_for, file="./Output/submission_Sn_VW.csv", row.names = FALSE)

#creating Model 10 csv
nn_for <- data.frame(date=date_for,load=round(forecast_nn$mean,digits=0))
write.csv(nn_for, file="./Output/submission_NN_VW.csv", row.names = FALSE)

#Model 11
SN_NN_for <- data.frame(date=date_for,load=round(SN_NN_fors=0))
write.csv(SN_NN_for, file="./Output/submission_SN_NN_VW.csv", row.names = FALSE)

#Model 12
STL_for1 <- data.frame(date=date_for,load=round(STL_for$mean,digits =0))
write.csv(SN_NN_for, file="./Output/submission_SN_NN_VW.csv", row.names = FALSE)


#creating Model 13 csv
GP_for <- data.frame(date=date_for,load=round(forecast_GP$mean,digits=0))
write.csv(nn_for, file="./Output/submission_GP_VW.csv", row.names = FALSE)

#  GP model12
Model_12 <- ksmooth(ts_load_train, "normal", bandwidth = 1)

# Forecast
forecast_GP <- predict(Model_12, x = seq_along(ts_load_train) + n_for)

# Plot forecast
plot(ts_load_train)
lines(seq_along(ts_load_train) + n_for, forecast_GP$y, col = "red")

```

```{r}
#Code chunck to copy to main documents

temp_load_data <- merge(temp, load, by = "date")
ts_temp_load <- ts(temp_load_data$daily_temp, start = start(temp_load_data$date), frequency = 12)
#decompose
ts_temp_load %>% mstl() %>%
  autoplot()



#creating subset for training purposes

n_for = 30

ts_temp_load_train <- subset(ts_temp_load,end = length(ts_temp_load)-n_for)
ts_temp_load_test <- subset(ts_temp_load,start= length(ts_temp_load)-n_for+1)

# Model15
Model15 <- snaive(ts_temp_load_train)
checkresiduals(Model15)

#Model 15: Seasonal naive
Model15 <- snaive(ts_temp_load_train,h=n_for+1)
summary(Model2)

plot(Model2)

###only competition submissions
#creating Model 15 csv
forecast_values <- as.numeric(ts_temp_load_train)
forecast_mean <- mean(forecast_values)
forecast_values <- as.numeric(ts_temp_load_train$mean)
model15_for <- data.frame(date = date_for, load = round(forecast_mean, digits = 0))
write.table(model15_for, file = "./Output/submission_model15_mean.csv", sep = ",", row.names = FALSE, col.names = FALSE)



#Model 16

# Split data into training and testing sets

train_data2 <- data2[1:100, ]
test_data2 <- data2[101:nrow(data2), ]

# Train forecasting models
model16A <- auto.arima(t1$target_variable1)
model16B <- auto.arima(train_data2$target_variable2)



# Forecast
Hum1 <- forecast(ts_humidity, h = n_for+1)
load1 <- forecast(ts_load_train, h = n_for+1)
# Example data (replace with your actual data)
data <- c(
    NA, 3986.490, NA, 3906.826, NA, 3623.044, NA, 4064.671, NA, 4288.480, NA, 4428.209, NA, 4400.410, NA, 4493.735, NA, 4441.833,
    NA, 4117.915, NA, 4250.049, NA, 4333.863, NA, 4294.096, NA, 4259.338, NA, 4200.482, NA, 4159.950, NA, 3949.760, NA, 4306.535,
    NA, 4481.709, NA, 4514.760, NA, 4493.602, NA, 4564.855, NA, 4545.191, NA, 4357.348, NA, 4707.352, NA, 4887.643, NA, 4828.035,
    NA, 4786.027, NA, 4591.777, NA, 4356.811, 63.23851, 3941.750, 60.69150, NA, 63.11973, NA, 66.43072, NA, 70.81117, NA, 71.83559,
    NA, 65.17322, NA, 69.26553, NA, 72.14252, NA, 74.40633, NA, 71.91633, NA, 71.19187, NA, 75.63596, NA, 71.22921, NA, 70.05341,
    NA, 70.27546, NA, 73.64641, NA, 72.69028, NA, 71.17887, NA, 70.66106, NA, 67.38333, NA, 70.10814, NA, 72.67083, NA, 73.07560,
    NA, 71.66364, NA, 70.04604, NA, 74.96890, NA, 73.58844, NA, 77.17688, NA, 72.97545, NA, 75.23126
)

# Separate data into two variables
Hum1_mean <- data[seq(1, length(data), by = 2)]  # Extract every other value
load1_mean <- data[seq(2, length(data), by = 2)] # Extract every other value starting from index 2

# Remove NA values
Hum1_mean <- Hum1_mean[!is.na(Hum1_mean)]
load1_mean <- load1_mean[!is.na(load1_mean)]

# Print the separated variables
print(Hum1_mean)
print(load1_mean)
combined_forecast <- cbind(Hum1_mean, load1_mean)
plot(combined_forecast)
combined_for <- data.frame(date=date_for,load=round(ETS_jul$mean,digits=0))
write.csv(ETS_for, file="./Output/submission_ets.csv", row.names = FALSE)
```

```{r}
#only competition submissions

# Weighted average of ETS and SN forecasts
combined_forecast23 <- (0.5 * ETS_jul$mean + 0.5 * SN_jul_for$mean)
test_f <- data.frame(date=date_for,load=round(combined_forecast23,digits=0))
colnames(test_f)[1] <- "date"
write.csv(test_f , file="./Output/submission_test.csv", row.names = FALSE)

#creating STL csv
test_f <- data.frame(date=date_for,load=round(combined_forecast23,digits=0))
colnames(test_f)[1] <- "date"
write.csv(test_f , file="./Output/submission_test.csv", row.names = FALSE)
```

```{r}
#only competition submissions
length(date_for)
length(combined_forecast_NNETS)

# Determine the length difference between ETS_jul and NN12_jul
length_diff <- length(NN12_jul) - length(ETS_jul)

# Extend ETS_jul to match the length of NN12_jul
ETS_jul_extended <- c(ETS_jul, rep(ETS_jul[length(ETS_jul)], length_diff))

# Calculate the mean of ETS_jul_extended
mean_ETS_jul_extended <- mean(ETS_jul_extended$mean)

# Calculate the weighted average of the extended ETS_jul and NN12_jul forecasts
combined_forecast_NNETS <- 0.5 * mean_ETS_jul_extended + 0.5 * NN12_jul$mean

model17 <- data.frame(date=date_for,load=round(combined_forecast_NNETS,digits=0))
colnames(test_f)[1] <- "date"
write.csv(test_f , file="./Output/submission_test.csv", row.names = FALSE)

#creating STL csv
test_f <- data.frame(date=date_for,load=round(combined_forecast23,digits=0))
colnames(test_f)[1] <- "date"
write.csv(test_f , file="./Output/submission_test.csv", row.names = FALSE)
```

```{r}
# Weighted average of ETS and SN forecasts
combined_forecast24 <- (0.5 * ETS_jul$mean + 0.5 * SN_jul_for$mean)
test_f1 <- data.frame(date=date_for,load=round(combined_forecast24,digits=0))
colnames(test_f1)[1] <- "date"
write.csv(test_f1 , file="./Output/submission_test.csv", row.names = FALSE)

#creating STL csv
test_f <- data.frame(date=date_for,load=round(combined_forecast23,digits=0))
colnames(test_f)[1] <- "date"
write.csv(test_f , file="./Output/submission_test.csv", row.names = FALSE)
```

```{r}
# Example of training a GBM model
gbm_model <- gbm(load ~ ., data = ts_load_train, 
                 distribution = "gaussian", n.trees = 100, interaction.depth = 3)

# Example of evaluating the GBM model
predictions <- predict(gbm_model, newdata = ts_load_train, n.trees = 100)
mse <- mean((predictions - ts_load_train)^2)

# Example of hyperparameter tuning
tuned_gbm_model <- gbm(target_variable ~ ., data = ts_load_train, 
                       distribution = "gaussian", n.trees = 100, 
                       interaction.depth = 3, 
                       cv.folds = 5, 
                       n.cores = NULL)
plot(tuned_gbm_model)
```

